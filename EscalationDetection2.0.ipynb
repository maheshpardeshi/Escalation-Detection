{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/swxdatascience/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] [Errno 13] Permission denied:\n",
      "[nltk_data]     '/home/swxdatascience/nltk_data/corpora/stopwords/slov\n",
      "[nltk_data]     ene'\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/swxdatascience/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/swxdatascience/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1106 06:57:59.235116 140401052120832 deprecation_wrapper.py:119] From /home/swxdatascience/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1106 06:57:59.251000 140401052120832 deprecation_wrapper.py:119] From /home/swxdatascience/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1106 06:57:59.257889 140401052120832 deprecation_wrapper.py:119] From /home/swxdatascience/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1106 06:57:59.823901 140401052120832 deprecation_wrapper.py:119] From /home/swxdatascience/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1106 06:57:59.833059 140401052120832 deprecation.py:506] From /home/swxdatascience/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1106 06:57:59.880244 140401052120832 deprecation_wrapper.py:119] From /home/swxdatascience/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1106 06:58:00.403742 140401052120832 deprecation_wrapper.py:119] From /home/swxdatascience/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1106 06:58:00.413662 140401052120832 deprecation.py:323] From /home/swxdatascience/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask,request\n",
    "#from flask_restful import Resource, Api\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from nltk.corpus import stopwords   \n",
    "import nltk\n",
    "import pandas as pd\n",
    "import spacy \n",
    "import simplejson as json\n",
    "from collections import defaultdict\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from html.parser import HTMLParser\n",
    "import re\n",
    "from nltk import RegexpParser\n",
    "from nltk.tree import Tree\n",
    "from applicationinsights.flask.ext import AppInsights\n",
    "import traceback\n",
    "import sys\n",
    "from applicationinsights import TelemetryClient\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from applicationinsights import channel\n",
    "from applicationinsights.logging import LoggingHandler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import keras.backend.tensorflow_backend as tb\n",
    "#Code to hanlde the exceptions and logs to App Insights in Azure\n",
    "tc = TelemetryClient('514ffbdf-cab4-4207-85fc-f3eb5c270d54')\n",
    "# set up channel with context\n",
    "telemetry_channel = channel.TelemetryChannel()\n",
    "telemetry_channel.context.application.ver = '1.0.0'\n",
    "telemetry_channel.context.properties['Intent'] = 'Escalation'\n",
    "\n",
    "#Set up logging\n",
    "handler = LoggingHandler('514ffbdf-cab4-4207-85fc-f3eb5c270d54', telemetry_channel=telemetry_channel)\n",
    "handler.setLevel(logging.DEBUG)\n",
    "handler.setFormatter(logging.Formatter('%(levelname)s: %(message)s'))\n",
    "logger = logging.getLogger('simple_logger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "#Log something (this will be sent to the Application Insights service as a trace)\n",
    "app = Flask(__name__)\n",
    "appinsights = AppInsights(app)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nnps=[\"Dear All\",\"Dear Team\",\"Dears\",\"Hi All\",\"Hi Team\",\"Hi\",\"Hey\",\"Hello All\",\"Hello Team\",\"Hi guys\"]\n",
    "NegativeSents=[]\n",
    "\n",
    "global graph\n",
    "graph = tf.compat.v1.get_default_graph()\n",
    "\n",
    "#Standford Core NLP Azure dockers URL to check the sentiment of the text\n",
    "nlps = StanfordCoreNLP('https://sentimentanalysis-dev-api.azurewebsites.net/')\n",
    "allPwps_Avns=[]\n",
    "filename = \"/var/www/EscalationDetection/LSTM_model_EscalationDetection.sav\"            \n",
    "filenameTok=\"/var/www/EscalationDetection/Escalation_tok.sav\"\n",
    "model = pickle.load(open(filename, 'rb'))\n",
    "tok=pickle.load(open(filenameTok, 'rb'))\n",
    "\n",
    "filename_Appr = \"/var/www/AppreciationDetection/Swx.SmartThing.Poc.Model.IntentDetection/LSTM_model_AppreciationDetection.sav\"            \n",
    "filenameTok_Appr=\"/var/www/AppreciationDetection/Swx.SmartThing.Poc.Model.IntentDetection/tok.sav\"\n",
    "\n",
    "model_Appr = pickle.load(open(filename_Appr, 'rb'))\n",
    "tok_Appr=pickle.load(open(filenameTok_Appr, 'rb'))\n",
    "\n",
    "max_words = 15000\n",
    "max_len = 300\n",
    "\n",
    "#Load Spacy model to get POS,Dependency matrix,lemma\n",
    "nlpSpacy = spacy.load('en_core_web_lg')\n",
    "#pronounsList=['you','me','myself','mine','your','yours','yourself','his','her','herself','he','she','they','them','who','whom','whose']\n",
    "pronounsList=['you','your','yours','yourself','his','her','herself','he','she','they','them','who','whom','whose','it','i','this']\n",
    "propNouns=['you','your']\n",
    "sampleSignatures=['best','regards','cheers','cheer','thanks','sincerely','thank you','thanks and regards','thank you  and  regards','thank you and regards','thanks & regards','yours']\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "work_related_nouns=['effort','review','work','worker','job','performance','perform','task','assignment','attempt','moil','achievement',\n",
    "'creation','go','energy','accomplishment','creation','success','victory','acquirement','acquisition','initiative',\n",
    "'action','stuff','hero','milestone','demonstration','workshop','sale','marketing','deal','support','documentation','observation',\n",
    "'contribution','achieved','achieve','idea','test','coverage','plan','doc','dedication','attitude','documentation','assistence','review',\n",
    "'achivement','test','testing','test case','test cases','testcase','employee','deployment','solution','developer','design','UI','designing',\n",
    "'handle','skill','implementation','creativity','delivery','help','supervision','progress','team','leader','colleague','guys','guy','lead',\n",
    "'quality','coworker','session','person','frontend','team','member','player','coder','knowledge','teamwork','result',\n",
    "'experience','buddy','approach','people','advice','enthusiasm','boy','project','man','woman','women','development','suggestion','report']\n",
    "\n",
    "mostlikelyApprAdjectives=['cool','great','awesome','nice','amazing','amazed','best','bright','brilliant','calm','carefull'\n",
    "                         'charm','clever','confident','congrats','enthusiastic','excellent','expert','fabulous','fantastic'\n",
    "                         'dignity','good','like','perfect','stromg','wonderfull','dedicate','wise','well','phenomenal',\n",
    "                         'Kudos','hardwork','consistent','positive','success','intelligent','ambition','creative','passion','efficient'\n",
    "                         'extraordinary','immense','cooperate','genius','progress','outstanding','pleasure','rockstar','asonishing'\n",
    "                         ]\n",
    "ADJ_Score_CSV=pd.read_csv(\"/var/www/EscalationDetection/PossWordsDF.csv\",encoding=\"ISO-8859-1\")\n",
    "ADJ_Score_DF=ADJ_Score_CSV[ADJ_Score_CSV['Sentiment']<1]\n",
    "ADJ_Score_DF.reset_index(inplace=True, drop=True)\n",
    "\n",
    "PosetiveADJ_Score_DF=ADJ_Score_CSV[ADJ_Score_CSV['Sentiment']>1]\n",
    "PosetiveADJ_Score_DF.reset_index(inplace=True, drop=True)\n",
    "\n",
    "app.config['APPINSIGHTS_INSTRUMENTATIONKEY'] = '514ffbdf-cab4-4207-85fc-f3eb5c270d54'\n",
    "replace_list = {r\"i'm\": 'i am',\n",
    "                r\"'re\": ' are',\n",
    "                r\"let's\": 'let us',                \n",
    "                r\"'ve\": ' have',\n",
    "                r\"can't\": 'can not',\n",
    "                r\"cannot\": 'can not',\n",
    "                r\"shan't\": 'shall not',\n",
    "                r\"n't\": ' not',\n",
    "                r\"'d\": ' would',\n",
    "                r\"'ll\": ' will',\n",
    "                r\"'scuse\": 'excuse',                \n",
    "                '!': '.',                \n",
    "                '\\s+': ' '}\n",
    "# To check is the text is imperative or non imperative text.\n",
    "def is_imperative(dep):\n",
    "    tagged_sent=[]\n",
    "    for k in dep:\n",
    "        temp=  [k[0],k[5],k[4]]\n",
    "        tagged_sent.append(tuple(temp))\n",
    "        \n",
    "    # if the sentence is not a question...\n",
    "    if len(tagged_sent)>0 and tagged_sent[-1][0] != \"?\":\n",
    "        # catches simple imperatives, e.g. \"Open the pod bay doors, HAL!\"\n",
    "        if(tagged_sent[0][2] in ['ADV']):\n",
    "            return False\n",
    "        if (tagged_sent[0][1] == \"VB\" or tagged_sent[0][1] == \"VBZ\" or tagged_sent[0][1] == \"VBP\" or \n",
    "        tagged_sent[0][1] == \"VBD\" or tagged_sent[0][1] == \"VBN\"  or tagged_sent[0][1] == \"MD\"):\n",
    "            return True\n",
    "\n",
    "        # catches imperative sentences starting with words like 'please', 'you',...\n",
    "        # E.g. \"Dave, stop.\", \"Just take a stress pill and think things over.\"\n",
    "        else:\n",
    "            chunk = get_chunks(tagged_sent)\n",
    "            # check if the first chunk of the sentence is a VB-Phrase\n",
    "            if type(chunk[0]) is Tree and chunk[0].label() == \"VB-Phrase\":\n",
    "                return True\n",
    "\n",
    "    # Questions can be imperatives too, let's check if this one is\n",
    "    else:\n",
    "        # check if sentence contains the word 'please'\n",
    "        pls = len([w for w in tagged_sent if w[0].lower() == \"please\"]) > 0\n",
    "        # catches requests disguised as questions\n",
    "        # e.g. \"Open the doors, HAL, please?\"\n",
    "        chunk = get_chunks(tagged_sent)\n",
    "        \n",
    "        if pls and (tagged_sent[0][1] == \"VB\"  or tagged_sent[0][1] == \"VBZ\" or tagged_sent[0][1] == \"VBP\" or \n",
    "        tagged_sent[0][1] == \"VBD\" or tagged_sent[0][1] == \"VBN\" or tagged_sent[0][1] == \"MD\"):\n",
    "            return True\n",
    "        \n",
    "        # catches imperatives ending with a Question tag\n",
    "        # and starting with a verb in base form, e.g. \"Stop it, will you?\"\n",
    "        elif type(chunk[-1]) is Tree and chunk[-1].label() == \"Q-Tag\":\n",
    "            if (chunk[0][1] == \"VB\" or\n",
    "                (type(chunk[0]) is Tree and chunk[0].label() == \"VB-Phrase\")):\n",
    "                return True\n",
    "            \n",
    "        elif (tagged_sent[0][1] == \"VB\"  or tagged_sent[0][1] == \"VBZ\" or tagged_sent[0][1] == \"VBP\" or\n",
    "              tagged_sent[0][1] == \"VBD\" or tagged_sent[0][1] == \"VBN\" or tagged_sent[0][1] == \"MD\"):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# chunks the sentence into grammatical phrases based on its POS-tags\n",
    "def get_chunks(tagged_sent):\n",
    "    chunkgram = r\"\"\"VB-Phrase: {<DT><,>*<VB>}\n",
    "                    VB-Phrase: {<RB><VB>}\n",
    "                    VB-Phrase: {<UH><,>*<VB>}\n",
    "                    VB-Phrase: {<UH><,><VBP>}\n",
    "                    VB-Phrase: {<PRP><VB>}\n",
    "                    VB-Phrase: {<NN.?>+<,>*<VB>}\n",
    "                    Q-Tag: {<,><MD><RB>*<PRP><.>*}\"\"\"\n",
    "    chunkparser = RegexpParser(chunkgram)\n",
    "    return chunkparser.parse(tagged_sent)\n",
    "\n",
    "# Find the signature start in the array of sentences.\n",
    "def getMailSignature(f,e):\n",
    "    if(f!='' and e!=''):\n",
    "        if(f.lower().strip().find(e.lower().strip())>=0):        \n",
    "            return True\n",
    "        elif(e.lower().strip().find(f.lower().strip())>=0):        \n",
    "            return True\n",
    "    return False   \n",
    "\n",
    "# Detect the sentence is action required sentence or else\n",
    "def getActionRequiredText(sentence,dep):\n",
    "    actionRequiredWords=['need','have to','has to','have been','must', 'ought', 'shall', 'should','will','might','if','can','would be']    \n",
    "    imp=True\n",
    "    #isimerativeSent=bool([s for s in sentence.split(' ') if s.lower() in actionRequiredWords])\n",
    "    isimerativeSent=bool([a for a in actionRequiredWords if sentence.strip().lower().find(a)>-1])\n",
    "    #isimerativeSent=any(word in sentence for word in actionRequiredWords)\n",
    "    for d in dep:\n",
    "        if(d[6].lower() in ['expect','hope','hopefully']):\n",
    "            return True\n",
    "    if(len(dep)>0 and dep[0][0].lower() in 'thank'):\n",
    "        return False\n",
    "    \n",
    "    if(not isimerativeSent): \n",
    "        isimerativeSent=is_imperative(dep)\n",
    "   \n",
    "    return isimerativeSent\n",
    "\n",
    "#app.config('ServerName','13.71.1.136')\n",
    "def getPositivityUsingSFCoreNLP(sent,dep):\n",
    "    sent=getPOSRemovedSent(dep)\n",
    "    try:        \n",
    "        res = nlps.annotate(sent,properties={'annotators': 'sentiment','outputFormat': 'json','timeout': 1000,})\n",
    "        for s in res[\"sentences\"]:\n",
    "            if(s[\"sentiment\"] =='Negative'):\n",
    "                sent=getPOSRemovedSent(dep,True)\n",
    "                res = nlps.annotate(sent,properties={'annotators': 'sentiment','outputFormat': 'json','timeout': 1000,})\n",
    "                for s in res[\"sentences\"]:\n",
    "                    return(s[\"sentiment\"])                    \n",
    "            return(s[\"sentiment\"])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#Check sentance tartgeted to user is present in list of employees or TO or CC employees\n",
    "def isKnownUser(empName,TOUsers,CCUsers):           \n",
    "    ToUsersList=TOUsers.split(\";\")\n",
    "    CCUsersList=CCUsers.split(\";\")    \n",
    "    for t in ToUsersList:\n",
    "        if(t.lower().find(empName.lower())>-1):\n",
    "            return t\n",
    "    for t in CCUsersList:        \n",
    "        if(t.lower().find(empName.lower())>-1):\n",
    "            return t\n",
    "    return []\n",
    "\n",
    "#check all words are proper nouns only\n",
    "def onlyPropNounsInSents(ent):\n",
    "    onlyPropNounsInSent=True \n",
    "    #Filter our word dear from mails starting    \n",
    "    ent=[e for e in ent if e[0].lower() not in ['dear']]\n",
    "    #Filter out injectives and determinent, Proper nouns and punct \n",
    "    for e in range(len(ent)): \n",
    "        if(ent[e][4] not in ['INTJ','DET','PROPN','PUNCT','CCONJ']):\n",
    "            onlyPropNounsInSent=False\n",
    "            break\n",
    "    return onlyPropNounsInSent\n",
    "\n",
    "#Find the name of targeted user in sentance\n",
    "def getTargetedUsers(en):        \n",
    "        users=[]\n",
    "        for i in range(len(en)):\n",
    "            if(i!=len(en)-1):       \n",
    "                pren=en[i-1]\n",
    "                fi=en[i]\n",
    "                si=en[i+1]  \n",
    "                if(fi[4]==si[4]=='PROPN'):\n",
    "                    users.append(fi[0]+\" \"+si[0])\n",
    "                    i=i+1   \n",
    "                elif(fi[4]=='PROPN' and fi[1]=='compound' and si[4]=='PROPN'):\n",
    "                    users.append(fi[0]+\" \"+fi[2])\n",
    "                    i=i+1   \n",
    "                elif(fi[4]==\"PROPN\" and si[4]!=\"PROPN\" and i>=1 and pren[4]!=\"PROPN\" and fi[0] not in users ):\n",
    "                    users.append(fi[0])\n",
    "                elif(fi[4]==\"PROPN\" and si[4]!=\"PROPN\" and i==0 and fi[0] not in users ):\n",
    "                    users.append(fi[0].replace(\"@\",\"\"))    \n",
    "            else:   \n",
    "                fi=en[i]\n",
    "                pren=en[i-1]                \n",
    "                if(fi[4]==\"PROPN\"  and i>=1 and pren[4]!=\"PROPN\"):\n",
    "                    users.append(fi[0])        \n",
    "                if(len(en)==1 and fi[4]==\"PROPN\"):\n",
    "                    users.append(fi[0])        \n",
    "        \n",
    "        return users  \n",
    "\n",
    "#Check is there any proper noun in sentance\n",
    "def check_PROPNPresent(ent,Mail_To,Mail_CC): \n",
    "    isPresent=[]\n",
    "    NotPresent=[]        \n",
    "    users=getTargetedUsers(ent)\n",
    "    for j in range(len(users)):\n",
    "        KnownUsers= isKnownUser(users[j],Mail_To,Mail_CC)\n",
    "        if(len(KnownUsers)>0):\n",
    "            isPresent.append(KnownUsers)\n",
    "        else:\n",
    "            NotPresent.append(users[j])   \n",
    "    return isPresent,NotPresent    \n",
    "    \n",
    "# Find the sentnece is talking about which PROPN (Proper Noun)    \n",
    "def findSentanceUserMapping(mailBody,mailFrom,mailTo,mailCC):\n",
    "    Sentance_TargetedUser=[]    \n",
    "    tf=\"\"    \n",
    "    tfu=\"\"\n",
    "    updatedSentAfterYou=\"\"\n",
    "    # Iterate over list of sentences in text. tf:targetedFor,tfu:targetedForIfYou\n",
    "    for m in mailBody:        \n",
    "        d = {}         \n",
    "        if(m.strip()!='' ):  \n",
    "            m=m.replace('@','').replace('&',' and ')                        \n",
    "            dep=getDependency(m,\"Escalation\")            \n",
    "            onlyPropNounsInSent=onlyPropNounsInSents(dep)\n",
    "            isPresent,NotPresent=check_PROPNPresent(dep,mailTo,mailCC)\n",
    "            #Find the proper noun and sentence mapping\n",
    "            if(len(isPresent)>0 ):                \n",
    "                tf=\";\".join([t for t in isPresent])                \n",
    "                if(onlyPropNounsInSent):                     \n",
    "                    tfu=tf\n",
    "            if(len(isPresent) is 0):\n",
    "                if(len(tf)==0):\n",
    "                    tf= \"\".join([t for t in mailTo])  \n",
    "\n",
    "            if(any(p in ['you','yours','yourself'] for p in m.lower())):\n",
    "                if(tfu!=''): \n",
    "                    tf=tfu\n",
    "                updatedSentAfterYou=m.replace('you',tf)\n",
    "                updatedSentAfterYou=m.replace('You',tf)\n",
    "            #if(' i ' in m.lower() and checkPronounInSent(m.lower())):\n",
    "            #    tf=mailFrom\n",
    "            d[tf] = m\n",
    "            Sentance_TargetedUser.append(json.dumps(d))            \n",
    "    return Sentance_TargetedUser\n",
    "\n",
    "# Check the pronouns in the sentance\n",
    "def checkPronounInSent(sent):\n",
    "    return bool([s for s in sent.split(' ') if s.strip().lower() in pronounsList])\n",
    "  \n",
    "\n",
    "#Tokanise the sentance\n",
    "def nltkToknisation(docs):        \n",
    "    doc = [w[0] for w in docs.values]\n",
    "    _docs = [[w.lower() for w in word_tokenize(text)] for text in doc]\n",
    "    return _docs\n",
    "\n",
    "#Find the dependency in the sentance\n",
    "def getDependency(sent,model):        \n",
    "    doc = nlpSpacy(sent)\n",
    "    dep=[]\n",
    "    adjs=[]\n",
    "    if model==\"Appreciation\":\n",
    "        adjs=ADJ_Score_DF['Text'].values\n",
    "    else:\n",
    "        adjs=PosetiveADJ_Score_DF['Text'].values\n",
    "    \n",
    "    for token in doc:\n",
    "        if(token.text.lower()  in adjs):            \n",
    "            dep.append([token.text, token.dep_, token.head.text, token.head.pos_,'ADJ',token.tag_,token.lemma_.lower()])\n",
    "        else:\n",
    "            dep.append([token.text, token.dep_, token.head.text, token.head.pos_,token.pos_,token.tag_,token.lemma_])\n",
    "    \n",
    "    return dep\n",
    "def filterDataType(sent):\n",
    "    types=['TIME','DATE']\n",
    "    doc = nlpSpacy(sent)\n",
    "    entities=[(i, i.label_, i.label) for i in doc.ents]\n",
    "    return not bool([e for e in entities if e[1] in ['TIME']])\n",
    "# Find the subject of the sentance\n",
    "def findSubjectInSent(dep):    \n",
    "    allSubs=[]\n",
    "    for i in range(len(dep)):\n",
    "        try:\n",
    "            if(dep[i][1]=='nsubj'):\n",
    "                 allSubs.append([dep[i][0],dep[i][4]])  \n",
    "            if(dep[i][1]=='poss' and dep[i][4]=='ADJ'):\n",
    "                allSubs.append([dep[i][0]])\n",
    "        except:        \n",
    "            pass\n",
    "    return allSubs\n",
    "\n",
    "# Get Part Of speach from the sentance\n",
    "def getPOSTag(dep,pos):\n",
    "    try:\n",
    "        if(pos in ['NOUN','VERB','PROPN']):  \n",
    "            return [x for x in dep if x[4] == pos][0][0]    \n",
    "        if(pos in ['ADJ']):        \n",
    "            return [x for x in dep if x[4] == pos and x[1]!='poss'][0][0]\n",
    "        if(pos in ['amod']):        \n",
    "            amod=[x for x in dep if x[1] == pos][0]    \n",
    "            return [amod[0][0],amod[0][2]]\n",
    "        if(pos in ['aux']):        \n",
    "            return [x for x in dep if x[1] == pos or x[4] ==pos][0][0]    \n",
    "        if(pos in ['PRON']):        \n",
    "            return [x for x in dep if len(x)>0 and(x[1] == pos or x[0].lower() =='your')][0][0]\n",
    "        if(pos in ['VBG']):        \n",
    "            return [x for x in dep if x[5] == pos][0][0]    \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Find the tesne of the sentances\n",
    "def getTenseOfSent(dep):\n",
    "    tense = {}\n",
    "    past=fut=pres=0   \n",
    "    \n",
    "    for i in range(len(dep)):         \n",
    "        if(dep[i][5] in [\"MD\",\"VBC\",\"VBF\"]):\n",
    "            fut=fut+1\n",
    "        if(dep[i][5] in [\"VBP\", \"VBZ\",\"VBG\",\"VB\"]):\n",
    "            pres=pres+1        \n",
    "        if(dep[i][5] in [\"VBD\", \"VBN\"]):\n",
    "            past=past+1\n",
    "        \n",
    "    tense[\"future\"] =fut\n",
    "    tense[\"present\"]=pres\n",
    "    tense[\"past\"] =past    \n",
    "    return json.dumps(tense)\n",
    "\n",
    "#Find the object in the element\n",
    "def find(l, elem):\n",
    "    for row, i in enumerate(l):\n",
    "        try:\n",
    "            column = i.index(elem)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        return row, column\n",
    "    return -1\n",
    "\n",
    "#Return the ith row from matrix\n",
    "def column(matrix, i):\n",
    "    return [row[i] for row in matrix]\n",
    "\n",
    "#To check is the sentence is not in future tense or requested sentence\n",
    "def isValiedText(sent,dep,mail_To,mail_CC,pwps):\n",
    "    tense=json.loads(getTenseOfSent(dep))   \n",
    "    _aux=0\n",
    "    aux=getPOSTag(dep,'aux')    \n",
    "    if(aux!='' and aux is not None):\n",
    "        _aux=len(aux)\n",
    "    vbg=getPOSTag(dep,'VBG')    \n",
    "    _vbg=0    \n",
    "    \n",
    "    #Get the gerund verb\n",
    "    if(vbg!='' and vbg is not None):\n",
    "        _vbg=len(vbg)\n",
    "    \n",
    "    #Find out is any proper nouns from TO or CC present in text    \n",
    "    presentUser=check_PROPNPresent(dep,mail_To,mail_CC)[0]\n",
    "    \n",
    "    #Get the text in requested form\n",
    "    if(any('please' in x.lower() for x in nltk.word_tokenize(sent))):\n",
    "        return False,presentUser\n",
    "    \n",
    "    #Returns false if text is in future tense \n",
    "    if(tense['future']>0 and tense['past']==0 and tense['present']>0 and _aux>0 and _vbg==0):    \n",
    "        return False,presentUser\n",
    "    \n",
    "    if(len(pwps)==0 and isAppreciateKeywordPresent(dep)):                \n",
    "        return False,presentUser\n",
    "    return True,presentUser\n",
    "\n",
    "#Get POS removed sentance for check sentance polarity\n",
    "def getPOSRemovedSent(dep,secondOpenion=False):\n",
    "    updatedSent=''    \n",
    "    removePOS=['PROPN','INTJ']\n",
    "    auxWords=['was','had','has','have','will','would','should']\n",
    "    IntjWords=['hi','dear','hello','hey','ah','oh','hmm','ouch','uh','just','really','everyone','too','quickly']\n",
    "    for i in range(len(dep)):        \n",
    "        if(dep[i][1] not in removePOS and dep[i][4] not in removePOS and dep[i][0].lower() not in IntjWords):         \n",
    "            updatedSent=updatedSent+' '+dep[i][0]\n",
    "        if(secondOpenion and dep[i][0].lower() not in auxWords):\n",
    "            updatedSent=updatedSent+' '+dep[i][0]\n",
    "            \n",
    "    updatedSent=updatedSent.replace(\"n't\",'not').replace(\"Well\",\"well\")\n",
    "    return updatedSent\n",
    "\n",
    "#Is positive words from the list of positive words present\n",
    "def isPossitiveWordPresent(dep,model):          \n",
    "    possWords=[]    \n",
    "    adjs=[]\n",
    "    if model==\"Appreciation\":        \n",
    "        for i in range(len(dep)):\n",
    "            lemma=dep[i][6]        \n",
    "            if(PosetiveADJ_Score_DF[PosetiveADJ_Score_DF['Text']==lemma].shape[0]>0):         \n",
    "                possWords.append([dep[i][0],PosetiveADJ_Score_DF[PosetiveADJ_Score_DF['Text']==lemma][['Text','Sentiment']].values[0][1]])\n",
    "    else:                \n",
    "        for i in range(len(dep)):\n",
    "            lemma=dep[i][6]        \n",
    "            if(lemma in ADJ_Score_CSV.Text.values):         \n",
    "                possWords.append([dep[i][0],ADJ_Score_CSV[ADJ_Score_CSV.Text ==lemma].Sentiment.values[0]])\n",
    "\n",
    "    return possWords\n",
    "\n",
    "#Find out is given text contain any commanly used adjectives \n",
    "def isCommanPositiveWordPresent(dep,model):          \n",
    "    possWords=[]\n",
    "    try:\n",
    "        if model==\"Appreciation\":    \n",
    "            for i in range(len(dep)):\n",
    "                lemma=dep[i][6]       \n",
    "                if(PosetiveADJ_Score_DF[PosetiveADJ_Score_DF['Text']==lemma].shape[0]>0):\n",
    "                    possWords.append([dep[i][0],PosetiveADJ_Score_DF[PosetiveADJ_Score_DF['Text']==lemma][['Text','Sentiment']].values[0][1]])\n",
    "        else:\n",
    "            for i in range(len(dep)):\n",
    "                lemma=dep[i][6]       \n",
    "                if(lemma in ADJ_Score_CSV.Text.values):         \n",
    "                    possWords.append([dep[i][0],ADJ_Score_CSV[ADJ_Score_CSV.Text ==lemma].Sentiment.values[0]])\n",
    "\n",
    "        return possWords\n",
    "    except:\n",
    "        return possWords\n",
    "\n",
    "#check is defined Escalation verbs peresent\n",
    "def isApprVerbNounPresent(dep):\n",
    "    appWords=[]\n",
    "    for i in range(len(dep)):\n",
    "        if(dep[i][6].lower() in work_related_nouns and (dep[i][4] in ['NOUN','PROPN','VERB','ADJ'] or dep[i][5] in ['VBG','NN']) ):\n",
    "            appWords.append(dep[i][0].lower())\n",
    "    return appWords\n",
    "\n",
    "def isAppreciateKeywordPresent(dep):\n",
    "    return bool([d for d in dep if d[6] in ['appreciate'] and d[1] not in ['compound']])\n",
    "\n",
    "#get the simple seneances in array from the complex sentance\n",
    "def seperateSent(sent,dep):    \n",
    "    independentSent=[]\n",
    "    Sentences=getIndependentSent(dep)\n",
    "    if(Sentences==[]):\n",
    "        Sentences.append(sent)\n",
    "    return Sentences\n",
    "\n",
    "#split the sentence by injector and punctuate \n",
    "def getIndependentSent(dep):\n",
    "    independentSent=[]    \n",
    "    newStart=False\n",
    "    words=[]\n",
    "    compoundSentences=[]\n",
    "    start=0\n",
    "    for i in range(len(dep)):\n",
    "        #Find the text with more than one subjects\n",
    "        if(column(dep,1).count('nsubj')>1):  \n",
    "            if(i==len(dep)-1 ):            \n",
    "                words.append(dep[i])\n",
    "                if(words!=[]):\n",
    "                    independentSent.append(words)  \n",
    "            #Seperate the sentence by punctuate and conjuction\n",
    "            if(((dep[i][1] == 'punct' and i>0 and dep[i-1][4] in ['PROPN']) or (dep[i][1] =='cc' and dep[i][0] not in ['and','or']) or dep[i][1]== 'mark') and i!=len(dep)-1 ):\n",
    "                newStart=True\n",
    "            if(newStart): \n",
    "                if(words!=[]):                    \n",
    "                    independentSent.append(words)\n",
    "                words=[]\n",
    "                newStart=False\n",
    "            if(newStart == False and dep[i][1] != 'punct' and dep[i][1] != 'mark' and i!=len(dep)-1 ):      \n",
    "                words.append(dep[i])            \n",
    "        else:          \n",
    "            if(column(dep,1).count('cc')>0 and column(dep,1).count('conj') >0):\n",
    "                if(dep[i][1] =='cc' and dep[i][0] not in ['and','or']):\n",
    "                    cSents=[]\n",
    "                    for k in range(start,i):                                                \n",
    "                        cSents.append(dep[k])                        \n",
    "                    start=i\n",
    "                    if(cSents!=[]):                        \n",
    "                        compoundSentences.append(cSents)\n",
    "                if(i==len(dep)-1):\n",
    "                    cSents=[]                    \n",
    "                    for k in range(start,i+1):                                                  \n",
    "                        cSents.append(dep[k])\n",
    "                    start=i\n",
    "                    if(cSents!=[]):\n",
    "                        compoundSentences.append(cSents)                        \n",
    "    if(len(words)>0):        \n",
    "        compoundSentences=independentSent\n",
    "    \n",
    "    if(len(compoundSentences)>0):\n",
    "        i=-1\n",
    "        while(i!=len(compoundSentences)):\n",
    "            i=i+1       \n",
    "            try:\n",
    "                conjPresent=False  \n",
    "                for j in range(len(compoundSentences[i])):            \n",
    "                    if(compoundSentences[i][j][1]=='conj' \n",
    "                       and (compoundSentences[i][j][3]=='PROPN'\n",
    "                       or compoundSentences[i][j][4]=='PROPN'\n",
    "                       or compoundSentences[i][j][4]=='PRON'  )):                                                      \n",
    "                        for k in range(len(compoundSentences[i])):                    \n",
    "                            compoundSentences[i-1].append(compoundSentences[i][k])\n",
    "                        del compoundSentences[i]\n",
    "                \n",
    "                if(i !=0 and column(compoundSentences[i],1).count('conj')>0):                    \n",
    "                    conjPresent=True\n",
    "\n",
    "                if(i !=0 and conjPresent==False):\n",
    "                    for k in range(len(compoundSentences[i])):                    \n",
    "                        compoundSentences[i-1].append(compoundSentences[i][k])\n",
    "                    del compoundSentences[i]\n",
    "                    i=i-1\n",
    "            except:                \n",
    "                 pass\n",
    "        sepSent=[]\n",
    "        #Join the seperate sentences if conjunct present\n",
    "        for i in range(len(compoundSentences)):\n",
    "                if(compoundSentences[i]!=[]): \n",
    "                    txt=''    \n",
    "                    for j in range(len(compoundSentences[i])):\n",
    "                        if(len(compoundSentences[i][j])>0 ):                             \n",
    "                            if((j==0 and compoundSentences[i][j][1] in ['cc'])):\n",
    "                                txt=txt+''\n",
    "                            else:\n",
    "                                txt=txt+' '+compoundSentences[i][j][0]                               \n",
    "                            \n",
    "                sepSent.append(txt)         \n",
    "   \n",
    "    if(len(compoundSentences)>0):\n",
    "        independentSent=sepSent\n",
    "    return independentSent\n",
    "\n",
    "#Detect the object about which we are talking in the text.\n",
    "def getObjectInSent(dep_in,presentUsers):    \n",
    "    entityNames=[]\n",
    "    if(len(dep_in)>1 and len(dep_in[1])>0):\n",
    "        for d in dep_in[1]:\n",
    "            if((d[4] in ['PROPN'] and d[4] not in presentUsers)):                \n",
    "                entityNames.append(d[0])\n",
    "    return entityNames\n",
    "\n",
    "#Find the sentences with thanks word\n",
    "def isThanksPresent(sent):\n",
    "    if(sent.lower().strip().find('thank you')>=0): \n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "    \n",
    "# Get complex sentence sepeated by inject\n",
    "def seperateSentenceByIN(dep):\n",
    "    inSeperatedSents=[]\n",
    "    inFirstPart=[]\n",
    "    inSecondPart=[]\n",
    "    inFound=False\n",
    "    for d in dep:        \n",
    "        if d[5] ==\"IN\":                \n",
    "            inFound=True\n",
    "        if(not inFound):            \n",
    "            inFirstPart.append(d)\n",
    "        else:\n",
    "            inSecondPart.append(d)\n",
    "    inSeperatedSents.append(inFirstPart)   \n",
    "    inSeperatedSents.append(inSecondPart)\n",
    "    return inSeperatedSents\n",
    "\n",
    "# Check adjective modifier present in sentence\n",
    "def isAdjModExist(dep,pwps):    \n",
    "    for i in range(len(dep)):        \n",
    "        if(i<len(dep)-1 and dep[i][1] in ['amod','advmod'] and dep[i+1][0].lower() in pwps):            \n",
    "            return True\n",
    "    return False\n",
    "\n",
    "#Check is the adjective noun present\n",
    "def OnlyAdjNounPairPresent(dep,model):    \n",
    "    dep=list(filter(lambda x : x[1] not in ['aux','det'], dep))\n",
    "    verb=column(dep,4).count('VERB')\n",
    "    noun=column(dep,4).count('NOUN')\n",
    "    adj=column(dep,4).count('ADJ')    \n",
    "    pron=[d[0] for d in dep if d[4]=='PRON' and d[0].lower() in propNouns]\n",
    "    df=pd.DataFrame(dep)\n",
    "    if(df.shape[0]>0):    \n",
    "        dep=[d for d in dep if d[4] not in ['PUNCT','CCONJ','PROPN']]\n",
    "    if(len(pron)>0):\n",
    "        dep=[d for d in dep if d[4] != 'PRON']\n",
    "    aprWord=False\n",
    "    if(len(dep)<4 and len(dep)>0):   \n",
    "        if(isCommanPositiveWordPresent(dep,model) and adj>0):    \n",
    "            if(len(dep)>1 and bool([d for d in dep if d[1] in ['compound']])):\n",
    "                return False\n",
    "            if(len(dep)==1):\n",
    "                aprWord=True\n",
    "            if(len(dep)==2 and (isApprVerbNounPresent(dep) or(verb==1 and adj==1))):\n",
    "                aprWord=True        \n",
    "    return aprWord\n",
    "\n",
    "#Add 0.5 score if more than one postive adjective present\n",
    "def adjustMultipleAdjectives(dep,escalationScore):    \n",
    "    for i in range(len(dep)):\n",
    "        if(i<len(dep)-1 and dep[i][4] == 'ADJ' and dep[i+1][4] =='ADJ'):            \n",
    "            escalationScore+=0.5\n",
    "    return escalationScore    \n",
    "\n",
    "#Find link in Preposition and adjective    \n",
    "def isPrepAdjPresent(dep,pwps):\n",
    "    pobjLst=[d for d in dep if(d[1] is 'pobj')]\n",
    "    adjLst=[d for d in dep if(d[4] is 'ADJ' and d[0].lower() in pwps)]\n",
    "    prepLst=[d for d in dep if(d[1] is 'prep')]\n",
    "    prepAdj=[p for p in prepLst if (p[2] in [p[0] for p in adjLst])]\n",
    "    objPrep=[o for o in pobjLst if o[2] in [p[0] for p in prepAdj]]\n",
    "    return bool([o for o in objPrep if o[0].lower() in pwps])\n",
    "\n",
    "#Find the dependency in verb and adjective e.g. it was PURE AWESOME\n",
    "def isVerbAppreciated(dep,pwps):    \n",
    "    verbLst=[d for d in dep if(d[4] is 'VERB')]\n",
    "    adjLst=[d for d in dep if(d[4] is 'ADJ' and d[0].lower() in pwps)]\n",
    "    subjLst=[d for d in dep if(d[1] is 'nsubj' and (d[0].lower() in pronounsList) or d[4] in ['PROPN'])]\n",
    "    verbAdj=[v for v in verbLst if ((v[0] in [a[2] for a in adjLst if a[1] in ['advmod','dobj','acomp']]))]\n",
    "    subjVerbList=[s for s in subjLst if(s[0].lower() in propNouns and s[2] in [v[0] for v in verbAdj])]    \n",
    "    if(len(subjVerbList)>0 and len(verbAdj)>0):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#Adjective should not be compound word.\n",
    "def isAdjCompoundWord(dep,pwps):\n",
    "    if(len([d for d in dep if d[4] =='ADJ' and d[0].lower() in pwps and d[1] in ['compound']])==len(pwps)):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def pobj_verb_adj(dep,pwps,avn):\n",
    "    verbLst=[d[0] for d in dep if(d[1] in ['ROOT','ccomp'] and d[4] in ['ADJ'] and d[6] in pwps)]\n",
    "    prepLst=[d[0] for d in dep if(d[1] is 'prep' and d[2] in verbLst)]\n",
    "    return bool([d for d in dep if d[0] in avn and d[1] in ['dobj','pobj'] and (d[2] in verbLst or d[2] in prepLst)])\n",
    "\n",
    "#You done your every job with full of passion;Well done job\n",
    "def avn_dobj_adj_verb(dep,pwps,avn):\n",
    "    rootItem=[r[0] for r in dep if r[1]=='ROOT' and r[4] in ['VERB']]\n",
    "    subjLst=[d[2] for d in dep if(d[1] is 'nsubj' and (d[0].lower() in pronounsList or d[4] in ['PROPN']) and d[2] in rootItem)]\n",
    "    avn_dobj=[d for d in dep if d[1] in d[1] in ['dobj'] and d[2] in subjLst and d[0] in avn]\n",
    "    if(len(avn_dobj)==0):\n",
    "        avn_dobj=[d for d in dep if d[1] in d[1] in ['dobj'] and d[0] in avn]    \n",
    "    adjLst=[d for d in dep if(d[4] is 'ADJ' and d[0].lower() in pwps)]\n",
    "    if(len(adjLst)>0 and len(avn_dobj)>0):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#Extract link in adjective and preposition.\n",
    "def verRoot_adjPobjOfPrep(dep,pwps):\n",
    "    try:\n",
    "        rootItem=[r for r in dep if r[1]=='ROOT' and r[4] in ['VERB']]\n",
    "        subjLst=[d for d in dep if(d[1] is 'nsubj' and d[2] ==rootItem[0][0] and (d[0].lower() in pronounsList or d[4] in ['PROPN']))]\n",
    "        adjLst=[d for d in dep if(d[4] is 'ADJ' and d[1] is 'pobj')]\n",
    "        prepLst=[d for d in dep if(d[1] is 'prep')]\n",
    "        if(len(subjLst)>0 and len(prepLst)>0 and adjLst[0][2] == prepLst[0][0] and adjLst[0][0].lower() in pwps):\n",
    "            return True\n",
    "        return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "#Get all the adjective and adjective complements and relations in them     \n",
    "def adj_acomp_root(dep,pwps,avn):\n",
    "    rootItem=[r for r in dep if r[1]=='ROOT' and r[4] in ['VERB']]    \n",
    "    adj_Acomp_Root=False  \n",
    "    if(len(rootItem)>0):\n",
    "        subjLst=[d for d in dep if(d[1] is 'nsubj' and d[2] ==rootItem[0][0] and (d[0].lower() in avn or d[0].lower() in pronounsList or d[4] in ['PROPN']))]\n",
    "        if(len(subjLst)>0):\n",
    "            adjLst=[d for d in dep if(d[4] is 'ADJ' and d[0].lower() in pwps)]\n",
    "            adj_Acomp_Root=[a for a in adjLst if a[1] in ['acomp','advmod'] and a[2].lower() in rootItem[0][0]]\n",
    "            if(adj_Acomp_Root==[] and len(adjLst)>0):                   \n",
    "                conjAdj= [d for d in dep if d[0] in [a[2] for a in adjLst if a[1] in ['conj']]]             \n",
    "                adj_Acomp_Root = [a for a in conjAdj if (a[1] in ['acomp','advmod'] and a[2].lower() in rootItem[0][0])]            \n",
    "    return bool(adj_Acomp_Root)\n",
    "\n",
    "#Find link in adjectvie and verb\n",
    "def attrAdj_verb(dep,pwps):\n",
    "    try:\n",
    "        verbLst=[d[0] for d in dep if(d[4] in ['VERB'] and d[1] in ['ROOT'])]\n",
    "        adjLst=[d for d in dep if(d[4] is 'ADJ' and d[1] in ['attr'] and d[2] in verbLst and d[0].lower() in pwps)]\n",
    "        subjLst=[d[0] for d in dep if(d[1] is 'nsubj' and d[2] in verbLst and (d[0].lower() in pronounsList or d[4] in ['PROPN']))]\n",
    "        if(len(subjLst)>0 and len(adjLst)>0):\n",
    "            return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "#Find out adjective modifiers relation present.\n",
    "def amod_adj(dep,pwps,avn):\n",
    "    try:\n",
    "        subLst=[d[0] for d in dep if d[1] in ['nsubj','ROOT']]        \n",
    "        amod_adjLst=[d for d in dep if d[4]=='ADJ' and d[1] in ['amod'] and d[2].lower() in avn and d[0].lower() in pwps]\n",
    "        return bool(amod_adjLst)\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "#Extract adjective conjuction     \n",
    "def conjAdjList(dep,pwps):\n",
    "    adjLst=[d for d in dep if(d[4] is 'ADJ' and d[1] in ['pobj','conj','acomp'] and d[0].lower() in pwps)]\n",
    "    rootItem=[r for r in dep if r[1]=='ROOT' and r[4] in ['VERB']]\n",
    "    conjItem=[a[2] for a in adjLst if a[1] =='conj']\n",
    "    subjLst=[d[0] for d in dep if(d[1] is 'nsubj' and (d[0].lower() in pronounsList or d[4] in ['PROPN']))]\n",
    "    conjRow=[]\n",
    "    if(conjItem!=[]):\n",
    "        conjRow=[d for d in dep if d[0]==conjItem[0]]\n",
    "    else:\n",
    "        conjRow=adjLst\n",
    "    if(len(conjRow)>0 and len(rootItem)>0 and conjRow[0][2] == rootItem[0][0] and len(subjLst)>0):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#Extract adjective objet relationship e.g. She showed a high level of technical skills\n",
    "def avn_pobj_adj_amod(dep,pwps,avn):\n",
    "    rootItem=[r for r in dep if r[1]=='ROOT' and r[4] in ['VERB']]\n",
    "    adjLst=[d for d in dep if(d[4] is 'ADJ' and d[1] in ['pobj','amod'] and d[0] in pwps)]\n",
    "    prepLst=[d[0] for d in dep if(d[1] is 'prep')]\n",
    "    pobjLst=[d for d in dep if d[0].lower() in avn and d[1] in ['pobj'] and d[2] in prepLst]\n",
    "    if(len(pobjLst)>0 and len(adjLst)>0 ):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#Extract adjective and noun relationship\n",
    "def attr_adj_noun(dep,pwps,avn):\n",
    "    try:\n",
    "        adjDep=[x for x in dep if x[0].lower() in pwps and x[1] == 'amod' and x[2].lower() in avn]      \n",
    "        attrLst=bool([d for d in dep if(d[0] in [a[2] for a in adjDep] and d[1] in ['attr'])])\n",
    "        if(len(attrLst)>0):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "        \n",
    "#Logic to get is the text has dependency matrix which proves that text is appreciated text or not. also cross check against LSTM to get extream appreciated texts and neutral text         \n",
    "def checkEscalationText(Appreciated_User,MailTo,MailCC,Mail_From,Sentence,MailBody):\n",
    "    dep=getDependency(Sentence,\"Escalation\")  \n",
    "    isAdjExist=column(dep,4).count('ADJ')   \n",
    "    #Check is the Escalation keywords present in the text\n",
    "    isApprKeyPresent=isAppreciateKeywordPresent(dep)\n",
    "    #Reject the sentences without any adjectives\n",
    "    if(isAdjExist>0):  \n",
    "        #Get the seperate simple sentences from the complex text.\n",
    "        sents=seperateSent(Sentence,dep)     \n",
    "        escalationDF=[]\n",
    "        for j in range(len(sents)):\n",
    "            escalationScore=0\n",
    "            isItEscalationText=False\n",
    "            sent=sents[j].strip()           \n",
    "            if(sent!='' and filterDataType(sent)):  \n",
    "                sent=re.sub(r\"[^a-zA-Z.,-]+\", ' ', sent).strip()\n",
    "                dep=getDependency(sent,\"Escalation\")    \n",
    "                #Get the imperative sentences           \n",
    "                isImperativeSent=getActionRequiredText(sent,dep)\n",
    "                onlyPropNounsInSent=onlyPropNounsInSents(dep)  \n",
    "                pwp=isPossitiveWordPresent(dep,\"Escalation\")\n",
    "                pwp=[p for p in pwp if p[1]<0]\n",
    "                #Do not Process further if isImperative is true or sentence only contains the proper nouns             \n",
    "                if(len(pwp)>0 and (not isImperativeSent and not onlyPropNounsInSent)):                    \n",
    "                    adj=column(dep,4).count('ADJ')\n",
    "                    if(adj>1):\n",
    "                        escalationScore=adjustMultipleAdjectives(dep,escalationScore)*-1                                            \n",
    "                    avn=isApprVerbNounPresent(dep)\n",
    "                    #print(\"escalationScore\",escalationScore)\n",
    "                    pwps=[]\n",
    "                    pwps=[p[0].lower() for p in pwp]                   \n",
    "                    if(len(pwps)>0 and set(avn)==set(pwps)):\n",
    "                        avn=[]\n",
    "                        avn_pwpNotSame=False\n",
    "                        \n",
    "                    if(len(pwp)>0):\n",
    "                        escalationScore+=pd.DataFrame(pwp).sort_values(1,ascending=False)[1].max()*-1\n",
    "                    akp=isAppreciateKeywordPresent(dep)\n",
    "                    \n",
    "                    pnp=IsPronPropnPresent(sent,dep,MailTo,MailCC)  \n",
    "                    #Get appreciated sentences with simple adjective noun pair\n",
    "                    adjn= OnlyAdjNounPairPresent(dep,\"Escalation\")\n",
    "\n",
    "                    #Get sentiment using Stanford core nlp\n",
    "                    sentiment=getPositivityUsingSFCoreNLP(sent,dep)  \n",
    "                    #Increase the score of Escalation by 0.5 if sentence is Positive or VeryPositive or Neutral.\n",
    "                    if((sentiment in ['Negative','Verynegative']) and (len(avn)>0 or len(pwp)>0)):\n",
    "                        escalationScore+=0.5\n",
    "                    if(sentiment in ['Positive','Verypositive']):\n",
    "                        escalationScore-=0.5\n",
    "                    isNegWordExist,presentUsers=isValiedText(sent,dep,MailTo,MailCC,pwps)\n",
    "                    \n",
    "                    #Increase score by 0.5 if adjective modifier present\n",
    "                    if(escalationScore>0):\n",
    "                        isItEscalationText=True\n",
    "                        if(isAdjModExist(dep,pwps)):\n",
    "                            escalationScore+=0.5\n",
    "                    adj=column(dep,4).count('ADJ') \n",
    "                    thanksWord=column(dep,6).count('thank') \n",
    "                    pobjects=[]\n",
    "                    isThanksExist=isThanksPresent(sent)\n",
    "                    \n",
    "                    if(adj>0):                    \n",
    "                        seperateSentenceByInject=seperateSentenceByIN(dep)\n",
    "                        pobjects=getObjectInSent(seperateSentenceByInject,presentUsers)\n",
    "                    \n",
    "                    \n",
    "                    #Reduce score for only thanks you text\n",
    "                    if(isThanksExist and len(pwp)==1):\n",
    "                        escalationScore=0\n",
    "                        isAppreciated=False                        \n",
    "                    seperateSentenceByInject=[]                    \n",
    "                    NegSentiment=sentiment in ['Negative','Verynegative']\n",
    "                    isNegativeWordPresent=False\n",
    "                    fixedEscalation=False\n",
    "                    \n",
    "                    #Find the dependency in the adjective and other POS\n",
    "                    pap=isPrepAdjPresent(dep,pwps)\n",
    "                    vap=isVerbAppreciated(dep,pwps)\n",
    "                    vrap=verRoot_adjPobjOfPrep(dep,pwps)\n",
    "                    cal=conjAdjList(dep,pwps)\n",
    "                    aap=amod_adj(dep,pwps,avn)\n",
    "                    aav=attrAdj_verb(dep,pwps)\n",
    "                    attr_adj_n=attr_adj_noun(dep,pwps,avn)\n",
    "                    acr=adj_acomp_root(dep,pwps,avn)\n",
    "                    ada=avn_dobj_adj_verb(dep,pwps,avn)\n",
    "                    pva=pobj_verb_adj(dep,pwps,avn)\n",
    "                    apam=avn_pobj_adj_amod(dep,pwps,avn)\n",
    "                    if( pap or vap or vrap or cal or aap or aav or attr_adj_n or acr or ada or pva or apam):\n",
    "                        fixedEscalation=True\n",
    "                    metadeta=[]\n",
    "                    \n",
    "                    isAdjComp=isAdjCompoundWord(dep,pwps)\n",
    "                    if(sentiment =='Negative'):\n",
    "                        NegativeSents.append(sent)\n",
    "                    if(sentiment in ['Negative','Neutral']):\n",
    "                        isNegativeWordPresent=bool([d for d in dep if(d[1] in ['neg'])])\n",
    "                    \n",
    "                    lstmScore=LSTMCheck(sent,\"Escalation\")\n",
    "                    \n",
    "                    lstmScore=float(lstmScore[0][0])\n",
    "                    if(escalationScore>=2 and lstmScore>0.6):\n",
    "                        escalationScore-=2\n",
    "                    if((fixedEscalation==False or escalationScore==0) and lstmScore<0.2):\n",
    "                        if(escalationScore<2):\n",
    "                            escalationScore+=2.1\n",
    "                        fixedEscalation=True    \n",
    "                    #Threshold score to between 0 and 5    \n",
    "                    if(escalationScore>5):\n",
    "                        escalationScore=5                        \n",
    "                    if(escalationScore<0):                        \n",
    "                        escalationScore=0\n",
    "                    #print(\"adjn\",adjn,\"lstmScore\",\"pwp\",pwp,lstmScore,\"escalationScore\",escalationScore,\"pap\",pap,\" vap\",vap,\"vrap\",vrap,\"cal\", cal,\" aap\", aap,\"aav\",aav,\"attr_adj_n\", attr_adj_n,\"acr\", acr,\"ada\",ada,\"pva\", pva,\"apam\", apam)\n",
    "                    if((NegSentiment or isNegativeWordPresent) and not isAdjComp):                        \n",
    "                        escalationDF.append([MailBody,Appreciated_User,Mail_From,MailTo,MailCC,sent,avn,pwp,akp,pnp,adjn,sentiment,escalationScore,isItEscalationText,pobjects,lstmScore])\n",
    "                    elif(adjn):\n",
    "                        escalationDF.append([MailBody,Appreciated_User,Mail_From,MailTo,MailCC,sent,avn,pwp,akp,pnp,adjn,sentiment,escalationScore,isItEscalationText,pobjects,lstmScore])\n",
    "                    \n",
    "                    elif((NegSentiment or isNegativeWordPresent) and fixedEscalation and isNegWordExist):\n",
    "                        escalationDF.append([MailBody,Appreciated_User,Mail_From,MailTo,MailCC,sent,avn,pwp,akp,pnp,adjn,sentiment,escalationScore,isItEscalationText,pobjects,lstmScore])                                                                \n",
    "                    \n",
    "                    if(not isThanksExist and escalationDF==[]):                        \n",
    "                        escalationDF.append([MailBody,Appreciated_User,Mail_From,MailTo,MailCC,sent,avn,pwp,akp,pnp,adjn,sentiment,escalationScore,isItEscalationText,pobjects,lstmScore])                        \n",
    "                    if(escalationDF==[]):                    \n",
    "                        escalationScore=0\n",
    "                        escalationDF.append([MailBody,Appreciated_User,Mail_From,MailTo,MailCC,sent,avn,pwp,akp,pnp,adjn,sentiment,escalationScore,False,pobjects,lstmScore])                    \n",
    "                    if(escalationScore>0 and not fixedEscalation and not adjn):                    \n",
    "                        isAppreciated=True\n",
    "                        compoundNounsDep = [x for x in dep if ((x[0] in avn or x[2] in avn) and x[1] == 'compound')]                       \n",
    "                        if(len(avn)==1 and len(compoundNounsDep)==1):\n",
    "                            isAppreciated=False\n",
    "                        if(len(avn)==0):\n",
    "                            isAppreciated=False                    \n",
    "                        if(isAppreciated and len(escalationDF)>0 and escalationDF[len(escalationDF)-1][5] == sent):\n",
    "                            escalationScore=0\n",
    "                            escalationDF[len(escalationDF)-1][13]=False\n",
    "                            escalationDF[len(escalationDF)-1][12]=0\n",
    "                    if(escalationScore>0):\n",
    "                        allPwps_Avns.append([escalationDF[len(escalationDF)-1][5],pwps,avn,escalationScore])                        \n",
    "        return escalationDF       \n",
    "    \n",
    "#check PRON or PROPN present or not \n",
    "def IsPronPropnPresent(sent,dep,mail_To,mail_CC):\n",
    "    isProNounPresent=any(x.lower() in propNouns for x in sent.split(' '))    \n",
    "    if(len(check_PROPNPresent(dep,mail_To,mail_CC)[0])==0 and not isProNounPresent):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "#Clean the text to remove html or javascript or any other tags.\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        # initialize the base class\n",
    "        HTMLParser.__init__(self)\n",
    "\n",
    "    def read(self, data):\n",
    "        # clear the current output before re-use\n",
    "        self._lines = []\n",
    "        # re-set the parser's state before re-use\n",
    "        self.reset()\n",
    "        self.feed(data)\n",
    "        return ''.join(self._lines)\n",
    "\n",
    "    def handle_data(self, d):\n",
    "        self._lines.append(d)\n",
    "\n",
    "\n",
    "#Find is the text conain any URL        \n",
    "def FindURL(string): \n",
    "    # findall() has been used  \n",
    "    # with valid conditions for urls in string \n",
    "    url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', string) \n",
    "    return url \n",
    "      \n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    return s.read(html)\n",
    "\n",
    "#Filter the text with unwanted characters or tags and split with \\n\n",
    "def clean_text(text):    \n",
    "    text=text.replace('\"', '')\n",
    "    text=text.replace(\"<br>\",\"\\n\")\n",
    "    text=re.sub('[^A-Za-z.,-]+\\<>*', ' ', strip_tags(text)).strip()\n",
    "    text=text.split(\"-----Original Message-----\")[0]         \n",
    "    for s in replace_list:        \n",
    "        text = text.replace(s, replace_list[s])    \n",
    "    \n",
    "    if(text.find(',')>-1):\n",
    "        i=text.index(\",\")\n",
    "        if(len(text[i])>i and text[i+1]=='\\n'):\n",
    "            text.replace(\",\",\"\")        \n",
    "    \n",
    "    text=text.replace(\"<div>\",\"\\\\n\").replace(\"</div>\",\"\").replace(\"&\",\" and \").replace(\"\\r\",\"\").replace('\\\\u2019',' a').replace(\"\\n\\n\",\"\\n\").replace(\"\\n \\n\",\"\\n\").replace(\"\\t\",\"\").replace(\"Thanks\",\"thank you\").replace(\"Thankx\",\"thank you\").replace(\"thanks\",\"thank you\").replace(\"-\",\"\").replace(\"\\\\n\",\"\\n\").replace('!','').replace(':)','great').replace('(:','bad').split(\"\\n\")\n",
    "    text=[m.strip() for m in text]\n",
    "    i=-1    \n",
    "    while(i!=len(text)):\n",
    "        i+=1\n",
    "        if(i<len(text)-1 and len(text[i+1])>0 and text[i]!='' and text[i][-1] !='.'):\n",
    "            text[i]+=' '+text[i+1]        \n",
    "            del text[i+1]\n",
    "            i=i-1\n",
    "    text=[m.strip() for m in text if(len(m)>3)]\n",
    "    \n",
    "    for k in range(len(text)):\n",
    "        if(k<len(text)-1 and text[k+1][0].islower()):\n",
    "            text[k]+=' '+text[k+1]\n",
    "            del text[k+1]\n",
    "    \n",
    "    return text\n",
    "\n",
    "#Check that text is not empty.\n",
    "def checkApiParameters(mailBody,MailFrom,MailTo,MailCC):\n",
    "    if type(MailFrom)!=str:\n",
    "        MailFrom=\"\"\n",
    "    if type(MailTo)!=str:\n",
    "        MailTo=\"\"\n",
    "    if type(MailCC) !=str:\n",
    "        MailCC=\"\"\n",
    "    if type(mailBody)!=str:\n",
    "        mailBody=\"\"\n",
    "    return mailBody,MailFrom,MailTo,MailCC\n",
    "\n",
    "#Filter the signature from the text\n",
    "def getSignatureSeperatedMail(mailBody,MailFrom):    \n",
    "    sigStart=[ele for ele in mailBody if(len(ele.strip()) >0 and (ele.strip().lower()[-1] is '.' \n",
    "                                                          and ele.strip().lower()[:-1]  in sampleSignatures) \n",
    "                                         or (ele.strip().lower()[:-1] is not '.' and ele.strip().lower()[:]  \n",
    "                                             in sampleSignatures))] \n",
    "    \n",
    "    if(len(sigStart)>0):\n",
    "        mailBody=mailBody[:mailBody.index(sigStart[len(sigStart)-1])]\n",
    "    fromUser=[e for e in mailBody if(getMailSignature(e,MailFrom) == True)]\n",
    "    fromUser=[f for f in fromUser if f!='']   \n",
    "    if(len(fromUser)>0):\n",
    "        mailBody=mailBody[:mailBody.index(fromUser[len(fromUser)-1])]\n",
    "    return mailBody\n",
    "\n",
    "#Calculate the average score against the multiple appreciated records.\n",
    "def getAverageScore(escalationsDF):\n",
    "    avgScore=0\n",
    "    if(escalationsDF.shape[0]>0):\n",
    "        avgScore=float(escalationsDF.loc[:,\"Escalation Score\"].values.max())\n",
    "        if(avgScore>3 and escalationsDF.loc[:,\"Escalation Score\"].values.sum()>=7):\n",
    "            avgScore=5\n",
    "        elif(avgScore>4 and escalationsDF.loc[:,\"Escalation Score\"].values.sum()<=7):\n",
    "            avgScore=4.5\n",
    "        elif(avgScore>4 and escalationsDF.loc[:,\"Escalation Score\"].values.sum()<=6):\n",
    "            avgScore=4\n",
    "    return avgScore\n",
    "\n",
    "#Calculate the average score against the multiple appreciated records.\n",
    "def getAverageAppreciationScore(appreciationDF):\n",
    "    avgScore=0\n",
    "    if(appreciationDF.shape[0]>0):\n",
    "        avgScore=float(appreciationDF.loc[:,\"Appreciation Score\"].values.max())\n",
    "        if(avgScore>3 and appreciationDF.loc[:,\"Appreciation Score\"].values.sum()>=7):\n",
    "            avgScore=5\n",
    "        elif(avgScore<4 and appreciationDF.loc[:,\"Appreciation Score\"].values.sum()>=7):\n",
    "            avgScore=4.5\n",
    "        elif(avgScore<4 and appreciationDF.loc[:,\"Appreciation Score\"].values.sum()>=6):\n",
    "            avgScore=4\n",
    "    return avgScore\n",
    "\n",
    "#Convert dataframe object to complex json object\n",
    "def DataframeToJSON(escalationsDF,avgScore,userScoreMapping):\n",
    "    jsonArray=[]\n",
    "    EscalationDataJson={}\n",
    "\n",
    "    if(escalationsDF.shape[0]>0):            \n",
    "        EscalationDataJson={\n",
    "        \"TextFrom\":escalationsDF.loc[0:0,\"MailFrom\"].values[0],\n",
    "        \"TextTo\":escalationsDF.loc[0:0,\"MailTO\"].values[0],\n",
    "        \"TextCC\":escalationsDF.loc[0:0,\"MailCC\"].values[0],\n",
    "        \"Text\":escalationsDF.loc[0:0,\"MailBody\"].values[0],\n",
    "        \"AverageScore\":avgScore\n",
    "        }    \n",
    "    \n",
    "    for i in range(escalationsDF.shape[0]):\n",
    "        sentData={}\n",
    "        sentData[\"EscalatedUser\"]=escalationsDF.loc[i:i,\"EscalatedUser\"].values[0]\n",
    "        sentData[\"EscalationScore\"]=float(escalationsDF.loc[i:i,\"Escalation Score\"].values[0])\n",
    "        sentData[\"EscalatedSentence\"]=escalationsDF.loc[i:i,\"Sentence\"].values[0]\n",
    "        sentData[\"isItEscalationText\"]=bool(escalationsDF.loc[i:i,\"isItEscalationText\"].values[0])\n",
    "        #sentData[\"IsAppreciated_ObjectPresent\"]=escalationsDF.loc[i:i,\"Object\"].values[0]\n",
    "        sentData[\"lstmScore\"]=escalationsDF.loc[i:i,\"lstmScore\"].values[0]\n",
    "        jsonArray.append(sentData)\n",
    "    RecipientUserScoreMapping=[]\n",
    "    for user in userScoreMapping:\n",
    "        userwiseScore={}\n",
    "        userwiseScore[\"RecipientUser\"]=user\n",
    "        averageUserScore=float(userScoreMapping[user])\n",
    "        if(float(userScoreMapping[user])>=7):\n",
    "            averageUserScore=5.0\n",
    "        if(float(userScoreMapping[user])>=5 and float(userScoreMapping[user])<7):\n",
    "            averageUserScore=4.0\n",
    "            \n",
    "        userwiseScore[\"EscalationScore\"]=float(averageUserScore)\n",
    "        RecipientUserScoreMapping.append(userwiseScore)\n",
    "    EscalationDataJson[\"EscalationDetails\"]=jsonArray\n",
    "    EscalationDataJson[\"RecipientUserEscalationDetails\"]=RecipientUserScoreMapping\n",
    "    \n",
    "    return EscalationDataJson\n",
    "\n",
    "#Read labled emails from the csv file for training the model. \n",
    "def readData():\n",
    "    df=pd.read_csv(\"IMDB_BravoDataSet.csv\")    \n",
    "    df=df[df.Text!=3]\n",
    "    df=df.dropna()\n",
    "    df=df.drop_duplicates()\n",
    "    df.reset_index(inplace = True)\n",
    "    df.drop(['index'],axis=1,inplace = True)\n",
    "    return df\n",
    "\n",
    "#Split data for train test model\n",
    "def splitData():\n",
    "    df=readData()\n",
    "    X = df.Text\n",
    "    Y = df.Label\n",
    "    le = LabelEncoder()\n",
    "    Y = le.fit_transform(Y)\n",
    "    Y = Y.reshape(-1,1)\n",
    "    \n",
    "    #Split data to train test objects\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15)    \n",
    "    tok = Tokenizer(num_words=max_words)\n",
    "    tok.fit_on_texts(X_train)\n",
    "    sequences = tok.texts_to_sequences(X_train)\n",
    "    sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "    return tok,sequences,sequences_matrix,Y_train\n",
    "\n",
    "#Initialise the layers for the LSTM model.\n",
    "def LSTMTraining():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model\n",
    "\n",
    "#Train the model using LSTM\n",
    "def trainModel():\n",
    "    tok,sequences,sequences_matrix,Y_train=splitData()\n",
    "\n",
    "    #save the tok file to disk for furhter use\n",
    "    pickle.dump(tok, open(filenameTok, 'wb'))\n",
    "    model = LSTMTraining()\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
    "    #Train model with training data\n",
    "    model.fit(sequences_matrix,Y_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "    return model\n",
    "\n",
    "#Save model for further use\n",
    "def saveModel(): \n",
    "    model=trainModel()  \n",
    "    #Save model to disk\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "#Classify the text using LSTM in appreciated or non appreciated text\n",
    "def LSTMCheck(Text,mod):\n",
    "    max_words = 15000\n",
    "    max_len = 300\n",
    "    \n",
    "    X_test=pd.Series([Text])\n",
    "    if(mod==\"Escalation\"):\n",
    "        test_sequences = tok.texts_to_sequences(X_test)\n",
    "        test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "    \n",
    "        #Predict the accuracy score on new text\n",
    "        with graph.as_default():\n",
    "            return str(model.predict(test_sequences_matrix)[0][0])\n",
    "    else:\n",
    "        test_sequences = tok_Appr.texts_to_sequences(X_test)\n",
    "        test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "    \n",
    "        #Predict the accuracy score on new text\n",
    "        with graph.as_default():\n",
    "            return str(model_Appr.predict(test_sequences_matrix)[0][0])\n",
    "        \n",
    "\n",
    "#Logic to get is the text has dependency matrix which proves that text is appreciated text or not. also cross check against LSTM to get extream appreciated texts and neutral text         \n",
    "def checkAppreciationText(Appreciated_User,MailTo,MailCC,Mail_From,Sentence,MailBody):\n",
    "    dep=getDependency(Sentence,\"Appreciation\")  \n",
    "    isAdjExist=column(dep,4).count('ADJ')   \n",
    "    \n",
    "    #Check is the appreciation keywords present in the text\n",
    "    isApprKeyPresent=isAppreciateKeywordPresent(dep)\n",
    "    \n",
    "    #Reject the sentences without any adjectives\n",
    "    if(isAdjExist>0 or isApprKeyPresent):        \n",
    "        #Get the seperate simple sentences from the complex text.\n",
    "        sents=seperateSent(Sentence,dep)        \n",
    "        ApprDF=[]\n",
    "        for j in range(len(sents)):\n",
    "            aprScore=0\n",
    "            isItAppriciationText=False\n",
    "            sent=sents[j].strip()           \n",
    "            if(sent!='' and filterDataType(sent)):  \n",
    "                sent=re.sub(r\"[^a-zA-Z.,-]+\", ' ', sent).strip()\n",
    "                dep=getDependency(sent,\"Appreciation\")\n",
    "\n",
    "                #Get the imperative sentences           \n",
    "                isImperativeSent=getActionRequiredText(sent,dep)\n",
    "\n",
    "                onlyPropNounsInSent=onlyPropNounsInSents(dep)   \n",
    "                #Do not Process further if isImperative is true or sentence only contains the proper nouns             \n",
    "                if((not isImperativeSent and not onlyPropNounsInSent) or isApprKeyPresent):\n",
    "                    adj=column(dep,4).count('ADJ') \n",
    "                    if(adj>1):\n",
    "                        aprScore=adjustMultipleAdjectives(dep,aprScore)                                            \n",
    "                    avn=isApprVerbNounPresent(dep)\n",
    "                    pwp=isPossitiveWordPresent(dep,\"Appreciation\")\n",
    "                    pwps=[]\n",
    "                    pwps=[p[0].lower() for p in pwp]\n",
    "                   \n",
    "                    if(len(pwps)>0 and set(avn)==set(pwps)):\n",
    "                        avn=[]\n",
    "                        avn_pwpNotSame=False\n",
    "                        \n",
    "                    if(len(pwp)>0):\n",
    "                        aprScore+=pd.DataFrame(pwp).sort_values(1,ascending=False)[1].max()\n",
    "                    akp=isAppreciateKeywordPresent(dep)\n",
    "                    pnp=IsPronPropnPresent(sent,dep,MailTo,MailCC)\n",
    "\n",
    "                    #Get appreciated sentences with simple adjective noun pair\n",
    "                    adjn= OnlyAdjNounPairPresent(dep,\"Appreciation\")\n",
    "\n",
    "                    #Get sentiment using Stanford core nlp\n",
    "                    sentiment=getPositivityUsingSFCoreNLP(sent,dep)  \n",
    "                                        \n",
    "                    #Increase the score of appreciation by 0.5 if sentence is Positive or VeryPositive or Neutral.\n",
    "                    if((sentiment in ['Positive','Verypositive','Neutral']) and (len(avn)>0 or len(pwp)>0 or akp)):\n",
    "                        aprScore+=0.5\n",
    "                    \n",
    "                    isPossWordExist,presentUsers=isValiedText(sent,dep,MailTo,MailCC,pwps)\n",
    "                    \n",
    "                    #Increase score by 0.5 if adjective modifier present\n",
    "                    if(aprScore>0):\n",
    "                        isItAppriciationText=True\n",
    "                        if(isAdjModExist(dep,pwps)):\n",
    "                            aprScore+=0.5\n",
    "                    \n",
    "                    adj=column(dep,4).count('ADJ') \n",
    "                    thanksWord=column(dep,6).count('thank') \n",
    "                    pobjects=[]\n",
    "                    isThanksExist=isThanksPresent(sent)\n",
    "                    #Reduce score for only thanks you text\n",
    "                    if(isThanksExist and len(pwp)==1):\n",
    "                        aprScore=0\n",
    "                        isAppreciated=False                        \n",
    "                        \n",
    "                    seperateSentenceByInject=[]                    \n",
    "                    posSentiment=sentiment in ['Positive','Verypositive','Neutral']\n",
    "                    isNegativeWordPresent=False\n",
    "                    fixedAppr=False\n",
    "                    \n",
    "                    #Find the dependency in the adjective and other POS\n",
    "                    pap=isPrepAdjPresent(dep,pwps)\n",
    "                    vap=isVerbAppreciated(dep,pwps)\n",
    "                    vrap=verRoot_adjPobjOfPrep(dep,pwps)\n",
    "                    cal=conjAdjList(dep,pwps)\n",
    "                    aap=amod_adj(dep,pwps,avn)\n",
    "                    aav=attrAdj_verb(dep,pwps)\n",
    "                    attr_adj_n=attr_adj_noun(dep,pwps,avn)\n",
    "                    acr=adj_acomp_root(dep,pwps,avn)\n",
    "                    ada=avn_dobj_adj_verb(dep,pwps,avn)\n",
    "                    pva=pobj_verb_adj(dep,pwps,avn)\n",
    "                    apam=avn_pobj_adj_amod(dep,pwps,avn)\n",
    "              \n",
    "                    \n",
    "                    if( pap or vap or vrap or cal or aap or aav or attr_adj_n or acr or ada or pva or apam):\n",
    "                        fixedAppr=True\n",
    "                    metadeta=[]\n",
    "                    \n",
    "                    isAdjComp=isAdjCompoundWord(dep,pwps)\n",
    "                    if(sentiment =='Negative'):\n",
    "                        NegativeSents.append(sent)\n",
    "                    if(sentiment in ['Negative','Neutral']):\n",
    "                        isNegativeWordPresent=bool([d for d in dep if(d[1] in ['neg'])])\n",
    "                    #print(\"sent\",sent)\n",
    "                    lstmScore=LSTMCheck(sent,\"Appreciation\")\n",
    "                    \n",
    "                    lstmScore=float(lstmScore)\n",
    "                    #print(\"lstmScore\",lstmScore)\n",
    "                    #print(\"lstmScore\",\"adjn\",adjn,\"pwp\",pwp,lstmScore,\"aprScore\",aprScore,\"pap\",pap,\" vap\",vap,\"vrap\",vrap,\"cal\", cal,\" aap\", aap,\"aav\",aav,\"attr_adj_n\", attr_adj_n,\"acr\", acr,\"ada\",ada,\"pva\", pva,\"apam\", apam)\n",
    "                    \n",
    "                    if(aprScore>=2 and lstmScore<0.2 and not akp):\n",
    "                        aprScore-=2\n",
    "                    if((fixedAppr==False or aprScore==0) and lstmScore>0.9):\n",
    "                        if(aprScore<2):\n",
    "                            aprScore+=2.1\n",
    "                        fixedAppr=True    \n",
    "                    if(akp and aprScore==2):\n",
    "                        aprScore+=0.5\n",
    "                    #Threshold score to between 0 and 5    \n",
    "                    if(aprScore>5):\n",
    "                        aprScore=5                        \n",
    "                    if(aprScore<0):                        \n",
    "                        aprScore=0\n",
    "                   \n",
    "                    if((posSentiment or not isNegativeWordPresent) and akp and not isAdjComp):                        \n",
    "                        ApprDF.append([MailBody,Appreciated_User,Mail_From,MailTo,MailCC,sent,avn,pwp,akp,pnp,adjn,sentiment,aprScore,isItAppriciationText,pobjects,lstmScore])\n",
    "                    elif(adjn):\n",
    "                        ApprDF.append([MailBody,Appreciated_User,Mail_From,MailTo,MailCC,sent,avn,pwp,akp,pnp,adjn,sentiment,aprScore,isItAppriciationText,pobjects,lstmScore])\n",
    "                    \n",
    "                    elif((posSentiment or not isNegativeWordPresent) and fixedAppr and isPossWordExist):\n",
    "                        ApprDF.append([MailBody,Appreciated_User,Mail_From,MailTo,MailCC,sent,avn,pwp,akp,pnp,adjn,sentiment,aprScore,isItAppriciationText,pobjects,lstmScore])                                                                \n",
    "                    \n",
    "                    if(isThanksExist and ApprDF==[]):                        \n",
    "                        ApprDF.append([MailBody,Appreciated_User,Mail_From,MailTo,MailCC,sent,avn,pwp,akp,pnp,adjn,sentiment,aprScore,isItAppriciationText,pobjects,lstmScore])                        \n",
    "                    if(ApprDF==[]):                    \n",
    "                        aprScore=0\n",
    "                        ApprDF.append([MailBody,Appreciated_User,Mail_From,MailTo,MailCC,sent,avn,pwp,akp,pnp,adjn,sentiment,aprScore,False,pobjects,lstmScore])                    \n",
    "                    if(aprScore>0 and not fixedAppr and not akp and not adjn):                    \n",
    "                        isAppreciated=True\n",
    "                        compoundNounsDep = [x for x in dep if ((x[0] in avn or x[2] in avn) and x[1] == 'compound')]                       \n",
    "                        if(len(avn)==1 and len(compoundNounsDep)==1):\n",
    "                            isAppreciated=False\n",
    "                        if(len(avn)==0):\n",
    "                            isAppreciated=False                    \n",
    "                        if(not isAppreciated and not isThanksExist and len(ApprDF)>0 and ApprDF[len(ApprDF)-1][5] == sent):\n",
    "                            aprScore=0\n",
    "                            ApprDF[len(ApprDF)-1][13]=False\n",
    "                            ApprDF[len(ApprDF)-1][12]=0\n",
    "                    if(aprScore>0):\n",
    "                        allPwps_Avns.append([ApprDF[len(ApprDF)-1][5],pwps,avn,aprScore])                        \n",
    "        return ApprDF  \n",
    "\n",
    "    \n",
    "def getAppreciationOrNotMatrix(ds,MailTo,MailCC,MailFrom,sentence,originalMail):        \n",
    "    try:\n",
    "        data=[]\n",
    "        #Iterate all the sentences in the mail\n",
    "        for j in range(len(ds)):            \n",
    "            for appreciated_Emp, sentence in json.loads(ds[j]).items(): \n",
    "                sentence=sentence.strip()                \n",
    "                appreciation=checkAppreciationText(appreciated_Emp,MailTo,MailCC,MailFrom,sentence,originalMail)\n",
    "                #detailedSents.extend(seperatedSents)\n",
    "                if(appreciation != []):\n",
    "                    data.append(appreciation)\n",
    "    \n",
    "        appreciationDF=pd.DataFrame()\n",
    "        for i in range(len(data)):       \n",
    "            appreciationDF=appreciationDF.append(pd.DataFrame(data[i],columns=['MailBody','AppreciatedUser','MailFrom','MailTO','MailCC', 'Sentence','avn','pwp','akp','pnp','adjn','Sentiment','Appreciation Score','IsItAppreciation','Object','lstmScore']))\n",
    "        \n",
    "        appreciationDF=appreciationDF.reset_index()        \n",
    "        avgScore=0.0\n",
    "        if(appreciationDF.shape[0]>0):\n",
    "            avgScore=getAverageAppreciationScore(appreciationDF)\n",
    "            \n",
    "        return avgScore\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.debug(traceback.print_exc())\n",
    "        tc.track_exception()\n",
    "        print(traceback.print_exc())\n",
    "        tc.flush()\n",
    "        logging.shutdown()\n",
    "        return 0  \n",
    "\n",
    "\n",
    "@app.route('/api/intent/Escalation', methods=['POST','GET'])\n",
    "def getEscalationOrNotMatrix(mailBody,MailFrom,MailTo,MailCC):        \n",
    "    try:\n",
    "        '''\n",
    "        if request.method == 'POST':\n",
    "            req_data = request.get_json(force=True)        \n",
    "            mailBody=req_data['Text']\n",
    "            MailFrom=req_data['TextFrom']\n",
    "            MailTo=req_data['TextTo']\n",
    "            MailCC=req_data['CC']    \n",
    "        else:    \n",
    "            parms=request.args.to_dict() \n",
    "            mailBody=parms['Text'] \n",
    "            MailFrom=parms['TextFrom'] \n",
    "            MailTo=parms['TextTo'] \n",
    "            MailCC=parms['CC']          \n",
    "        '''\n",
    "        log='Escalation Detection API Parameters: Text:'+str(mailBody)+\" TO:\"+str(MailTo)+\" CC:\"+str(MailCC)\n",
    "        logger.debug(log)\n",
    "        originalMail=mailBody    \n",
    "\n",
    "        #Clean the parameters which are empty        \n",
    "        mailBody,MailFrom,MailTo,MailCC=checkApiParameters(mailBody,MailFrom,MailTo,MailCC)               \n",
    "        mailBody=clean_text(mailBody) \n",
    "       \n",
    "        detailedSents=[]\n",
    "        escalationDF=data=Escalation=mailSignatures=[]        \n",
    "        \n",
    "        #Filter out the signature from the text\n",
    "        mailBody=getSignatureSeperatedMail(mailBody,MailFrom)\n",
    "        \n",
    "        for i in range(len(mailBody)):   \n",
    "            url=FindURL(mailBody[i])            \n",
    "            if(len(url)>0):\n",
    "                mailBody[i]=mailBody[i].replace(url[0],'')\n",
    "            seperatedSents=mailBody[i].split('.')\n",
    "       \n",
    "            for i in range(len(seperatedSents)):            \n",
    "                if(len(seperatedSents)>i+1 and len(seperatedSents[i+1])<3):\n",
    "                    seperatedSents[i]+=seperatedSents[i+1]\n",
    "            detailedSents.extend(seperatedSents)\n",
    "            \n",
    "        #Extract the detailsed structer of the text including targeted user.\n",
    "        ds=findSentanceUserMapping(detailedSents,MailFrom,MailTo,MailCC)    \n",
    "        EscalationDataJson={}\n",
    "        \n",
    "        \n",
    "            #Iterate all the sentences in the mail\n",
    "        for j in range(len(ds)):          \n",
    "            for appreciated_Emp, sentence in json.loads(ds[j]).items(): \n",
    "                sentence=sentence.strip()    \n",
    "                appreciationScore=getAppreciationOrNotMatrix(ds,MailTo,MailCC,MailFrom,sentence,originalMail)\n",
    "                #print(\"appreciationScore\",appreciationScore)\n",
    "                if(appreciationScore<2):\n",
    "                    Escalation=checkEscalationText(appreciated_Emp,MailTo,MailCC,MailFrom,sentence,originalMail)\n",
    "                    detailedSents.extend(seperatedSents)       \n",
    "                    if(Escalation != []):\n",
    "                        data.append(Escalation)\n",
    "        \n",
    "        escalationsDF=pd.DataFrame()\n",
    "        for i in range(len(data)):\n",
    "            escalationsDF=escalationsDF.append(pd.DataFrame(data[i],columns=['MailBody','EscalatedUser','MailFrom','MailTO','MailCC', 'Sentence','avn','pwp','akp','pnp','adjn','Sentiment','Escalation Score','isItEscalationText','Object','lstmScore']))\n",
    "        \n",
    "        escalationsDF=escalationsDF.reset_index()\n",
    "            \n",
    "        groupedScore={}\n",
    "        avgScore=0.0\n",
    "        userScoreMapping={}\n",
    "        \n",
    "        if(escalationsDF.shape[0]>0):\n",
    "            avgScore=getAverageScore(escalationsDF)\n",
    "\n",
    "            #Group by userwise score and get sum of score\n",
    "            groupedScore=escalationsDF.groupby(['EscalatedUser'])['Escalation Score'].sum()\n",
    "            \n",
    "            for names,score in groupedScore.iteritems():\n",
    "                name=names.split(\";\")\n",
    "                for n in name:\n",
    "                    if n.strip() in userScoreMapping:\n",
    "                        userScoreMapping[n.strip()]=userScoreMapping[n.strip()]+score\n",
    "                    else:\n",
    "                        userScoreMapping[n.strip()]=score\n",
    "        EscalationDataJson=DataframeToJSON(escalationsDF,avgScore,userScoreMapping)\n",
    "        logger.debug(\"Response Returned Success\")\n",
    "        tc.flush()\n",
    "        return json.dumps(EscalationDataJson)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.debug(traceback.print_exc())\n",
    "        tc.track_exception()\n",
    "        tc.flush()\n",
    "        logging.shutdown()\n",
    "        return json.dumps({\"Error\":\"Error has occured, Contact to Administrator.\"})\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    app.run(debug=False, host='0.0.0.0',threaded=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1106 06:58:26.611878 140401052120832 <ipython-input-1-f0763dfa59a6>:1386] Escalation Detection API Parameters: Text:If every colleague was like you, no one would complain about coming to work. I appreciate your can-do spirit and your belief in teamwork. TO: CC:\n",
      "I1106 06:58:30.626411 140401052120832 <ipython-input-1-f0763dfa59a6>:1451] Response Returned Success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"EscalationDetails\": [], \"RecipientUserEscalationDetails\": []}'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getEscalationOrNotMatrix(\"If every colleague was like you, no one would complain about coming to work. I appreciate your can-do spirit and your belief in teamwork.\",\"\",\"\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1105 13:45:31.612571 140121993873152 <ipython-input-2-18a149ffdc20>:1386] Escalation Detection API Parameters: Text:Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in. TO: CC:\n",
      "I1105 13:46:10.722247 140121993873152 <ipython-input-2-18a149ffdc20>:1451] Response Returned Success\n",
      "I1105 13:46:10.723555 140121993873152 <ipython-input-2-18a149ffdc20>:1386] Escalation Detection API Parameters: Text:This is an example of why the majority of action films are the same. Generic and boring, there's really nothing worth watching here. A complete waste of the then barely-tapped talents of Ice-T and Ice Cube, who've each proven many times over that they are capable of acting, and acting well. Don't bother with this one, go see New Jack City, Ricochet or watch New York Undercover for Ice-T, or Boyz n the Hood, Higher Learning or Friday for Ice Cube and see the real deal. Ice-T's horribly cliched dialogue alone makes this film grate at the teeth, and I'm still wondering what the heck Bill Paxton was doing in this film? And why the heck does he always play the exact same character? From Aliens onward, every film I've seen with Bill Paxton has him playing the exact same irritating character, and at least in Aliens his character died, which made it somewhat gratifying...<br /><br />Overall, this is second-rate action trash. There are countless better films to see, and if you really want to see this one, watch Judgement Night, which is practically a carbon copy but has better acting and a better script. The only thing that made this at all worth watching was a decent hand on the camera - the cinematography was almost refreshing, which comes close to making up for the horrible film itself - but not quite. 4/10. TO: CC:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"TextFrom\": \"\", \"TextTo\": \"\", \"TextCC\": \"\", \"Text\": \"Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\", \"AverageScore\": 5, \"EscalationDetails\": [{\"EscalatedUser\": \"\", \"EscalationScore\": 3.6, \"EscalatedSentence\": \"Costner has dragged out a movie for far longer than necessary\", \"isItEscalationText\": true, \"lstmScore\": 0.0}, {\"EscalatedUser\": \"\", \"EscalationScore\": 3.6, \"EscalatedSentence\": \"Most of us have ghosts in the closet and Costner s character are realized early on and then forgotten until much later by which time I did not care\", \"isItEscalationText\": true, \"lstmScore\": 0.0}, {\"EscalatedUser\": \"\", \"EscalationScore\": 3.6, \"EscalatedSentence\": \"The problem is he comes off as kid who thinks he s better than anyone else around him and shows no signs of a cluttered closet\", \"isItEscalationText\": true, \"lstmScore\": 0.0}, {\"EscalatedUser\": \"\", \"EscalationScore\": 2.5, \"EscalatedSentence\": \"His only obstacle appears to be winning over Costner\", \"isItEscalationText\": true, \"lstmScore\": 0.0}, {\"EscalatedUser\": \"\", \"EscalationScore\": 3.1, \"EscalatedSentence\": \"Finally when we are well past the half way point of this stinker Costner tells us all about Kutcher s ghosts\", \"isItEscalationText\": true, \"lstmScore\": 0.0}, {\"EscalatedUser\": \"\", \"EscalationScore\": 3.6, \"EscalatedSentence\": \"We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing\", \"isItEscalationText\": true, \"lstmScore\": 0.0}], \"RecipientUserEscalationDetails\": [{\"RecipientUser\": \"\", \"EscalationScore\": 5.0}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1105 13:46:48.272542 140121993873152 <ipython-input-2-18a149ffdc20>:1451] Response Returned Success\n",
      "I1105 13:46:48.273858 140121993873152 <ipython-input-2-18a149ffdc20>:1386] Escalation Detection API Parameters: Text:First of all I hate those moronic rappers, who could'nt act if they had a gun pressed against their foreheads. All they do is curse and shoot each other and acting like cliché'e version of gangsters.<br /><br />The movie doesn't take more than five minutes to explain what is going on before we're already at the warehouse There is not a single sympathetic character in this movie, except for the homeless guy, who is also the only one with half a brain.<br /><br />Bill Paxton and William Sadler are both hill billies and Sadlers character is just as much a villain as the gangsters. I did'nt like him right from the start.<br /><br />The movie is filled with pointless violence and Walter Hills specialty: people falling through windows with glass flying everywhere. There is pretty much no plot and it is a big problem when you root for no-one. Everybody dies, except from Paxton and the homeless guy and everybody get what they deserve.<br /><br />The only two black people that can act is the homeless guy and the junkie but they're actors by profession, not annoying ugly brain dead rappers.<br /><br />Stay away from this crap and watch 48 hours 1 and 2 instead. At lest they have characters you care about, a sense of humor and nothing but real actors in the cast. TO: CC:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"TextFrom\": \"\", \"TextTo\": \"\", \"TextCC\": \"\", \"Text\": \"This is an example of why the majority of action films are the same. Generic and boring, there's really nothing worth watching here. A complete waste of the then barely-tapped talents of Ice-T and Ice Cube, who've each proven many times over that they are capable of acting, and acting well. Don't bother with this one, go see New Jack City, Ricochet or watch New York Undercover for Ice-T, or Boyz n the Hood, Higher Learning or Friday for Ice Cube and see the real deal. Ice-T's horribly cliched dialogue alone makes this film grate at the teeth, and I'm still wondering what the heck Bill Paxton was doing in this film? And why the heck does he always play the exact same character? From Aliens onward, every film I've seen with Bill Paxton has him playing the exact same irritating character, and at least in Aliens his character died, which made it somewhat gratifying...<br /><br />Overall, this is second-rate action trash. There are countless better films to see, and if you really want to see this one, watch Judgement Night, which is practically a carbon copy but has better acting and a better script. The only thing that made this at all worth watching was a decent hand on the camera - the cinematography was almost refreshing, which comes close to making up for the horrible film itself - but not quite. 4/10.\", \"AverageScore\": 5, \"EscalationDetails\": [{\"EscalatedUser\": \"\", \"EscalationScore\": 3.5, \"EscalatedSentence\": \"Generic and boring , there s really nothing worth watching here\", \"isItEscalationText\": true, \"lstmScore\": 0.0}, {\"EscalatedUser\": \"\", \"EscalationScore\": 2.0, \"EscalatedSentence\": \"A complete waste of the then barelytapped talents of IceT and Ice Cube who have each proven many times\", \"isItEscalationText\": true, \"lstmScore\": 0.0}, {\"EscalatedUser\": \"\", \"EscalationScore\": 2.5, \"EscalatedSentence\": \"IceT s horribly cliched dialogue alone makes this film grate at the teeth and I m still wondering what the heck Bill Paxton was doing in this film\", \"isItEscalationText\": true, \"lstmScore\": 0.0}, {\"EscalatedUser\": \"\", \"EscalationScore\": 2.5, \"EscalatedSentence\": \"The only thing that made this at all worth watching was a decent hand on the camera the cinematography was almost refreshing which comes close to making up for the horrible film itself\", \"isItEscalationText\": true, \"lstmScore\": 0.0}, {\"EscalatedUser\": \"\", \"EscalationScore\": 4.0, \"EscalatedSentence\": \"not quite\", \"isItEscalationText\": true, \"lstmScore\": 0.0}], \"RecipientUserEscalationDetails\": [{\"RecipientUser\": \"\", \"EscalationScore\": 5.0}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1105 13:47:42.836129 140121993873152 <ipython-input-2-18a149ffdc20>:1451] Response Returned Success\n",
      "I1105 13:47:42.837495 140121993873152 <ipython-input-2-18a149ffdc20>:1386] Escalation Detection API Parameters: Text:Not even the Beatles could write songs everyone liked, and although Walter Hill is no mop-top he's second to none when it comes to thought provoking action movies. The nineties came and social platforms were changing in music and film, the emergence of the Rapper turned movie star was in full swing, the acting took a back seat to each man's overpowering regional accent and transparent acting. This was one of the many ice-t movies i saw as a kid and loved, only to watch them later and cringe. Bill Paxton and William Sadler are firemen with basic lives until a burning building tenant about to go up in flames hands over a map with gold implications. I hand it to Walter for quickly and neatly setting up the main characters and location. But i fault everyone involved for turning out Lame-o performances. Ice-t and cube must have been red hot at this time, and while I've enjoyed both their careers as rappers, in my opinion they fell flat in this movie. It's about ninety minutes of one guy ridiculously turning his back on the other guy to the point you find yourself locked in multiple states of disbelief. Now this is a movie, its not a documentary so i wont waste my time recounting all the stupid plot twists in this movie, but there were many, and they led nowhere. I got the feeling watching this that everyone on set was sord of confused and just playing things off the cuff. There are two things i still enjoy about it, one involves a scene with a needle and the other is Sadler's huge 45 pistol. Bottom line this movie is like domino's pizza. Yeah ill eat it if I'm hungry and i don't feel like cooking, But I'm well aware it tastes like crap. 3 stars, meh. TO: CC:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"TextFrom\": \"\", \"TextTo\": \"\", \"TextCC\": \"\", \"Text\": \"First of all I hate those moronic rappers, who could'nt act if they had a gun pressed against their foreheads. All they do is curse and shoot each other and acting like clich\\u00e9'e version of gangsters.<br /><br />The movie doesn't take more than five minutes to explain what is going on before we're already at the warehouse There is not a single sympathetic character in this movie, except for the homeless guy, who is also the only one with half a brain.<br /><br />Bill Paxton and William Sadler are both hill billies and Sadlers character is just as much a villain as the gangsters. I did'nt like him right from the start.<br /><br />The movie is filled with pointless violence and Walter Hills specialty: people falling through windows with glass flying everywhere. There is pretty much no plot and it is a big problem when you root for no-one. Everybody dies, except from Paxton and the homeless guy and everybody get what they deserve.<br /><br />The only two black people that can act is the homeless guy and the junkie but they're actors by profession, not annoying ugly brain dead rappers.<br /><br />Stay away from this crap and watch 48 hours 1 and 2 instead. At lest they have characters you care about, a sense of humor and nothing but real actors in the cast.\", \"AverageScore\": 5, \"EscalationDetails\": [{\"EscalatedUser\": \"\", \"EscalationScore\": 3.6, \"EscalatedSentence\": \"First of all I hate those moronic rappers who could nt act they had a gun pressed against their foreheads\", \"isItEscalationText\": true, \"lstmScore\": 0.0}, {\"EscalatedUser\": \"\", \"EscalationScore\": 3.6, \"EscalatedSentence\": \"All they do is curse and shoot each other and acting like clich e version of gangsters\", \"isItEscalationText\": true, \"lstmScore\": 0.0}, {\"EscalatedUser\": \"\", \"EscalationScore\": 3.0, \"EscalatedSentence\": \"The movie is filled with pointless violence and Walter Hills specialty people falling through windows with glass flying everywhere\", \"isItEscalationText\": true, \"lstmScore\": 0.0}, {\"EscalatedUser\": \"\", \"EscalationScore\": 2.0, \"EscalatedSentence\": \"There is pretty much no plot and it is a big problem when you root for noone\", \"isItEscalationText\": true, \"lstmScore\": 0.0}, {\"EscalatedUser\": \"\", \"EscalationScore\": 2.5, \"EscalatedSentence\": \"Everybody dies, except from Paxton and the homeless guy and everybody get what they deserve\", \"isItEscalationText\": true, \"lstmScore\": 0.0}, {\"EscalatedUser\": \"\", \"EscalationScore\": 2.5, \"EscalatedSentence\": \"they are actors by profession not annoying ugly brain dead rappers\", \"isItEscalationText\": true, \"lstmScore\": 0.0}], \"RecipientUserEscalationDetails\": [{\"RecipientUser\": \"\", \"EscalationScore\": 5.0}]}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-50789e418dc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnegMessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegMessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegMessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetEscalationOrNotMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegMessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-18a149ffdc20>\u001b[0m in \u001b[0;36mgetEscalationOrNotMatrix\u001b[0;34m(mailBody, MailFrom, MailTo, MailCC)\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mappreciated_Emp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m                 \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1419\u001b[0;31m                 \u001b[0mappreciationScore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetAppreciationOrNotMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMailTo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMailCC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMailFrom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moriginalMail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1420\u001b[0m                 \u001b[0;31m#print(\"appreciationScore\",appreciationScore)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappreciationScore\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-18a149ffdc20>\u001b[0m in \u001b[0;36mgetAppreciationOrNotMatrix\u001b[0;34m(ds, MailTo, MailCC, MailFrom, sentence, originalMail)\u001b[0m\n\u001b[1;32m   1341\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mappreciated_Emp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m                 \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m                 \u001b[0mappreciation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckAppreciationText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappreciated_Emp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMailTo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMailCC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMailFrom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moriginalMail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m                 \u001b[0;31m#detailedSents.extend(seperatedSents)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappreciation\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-18a149ffdc20>\u001b[0m in \u001b[0;36mcheckAppreciationText\u001b[0;34m(Appreciated_User, MailTo, MailCC, Mail_From, Sentence, MailBody)\u001b[0m\n\u001b[1;32m   1204\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m''\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfilterDataType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m                 \u001b[0msent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"[^a-zA-Z.,-]+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m                 \u001b[0mdep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetDependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Appreciation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                 \u001b[0;31m#Get the imperative sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-18a149ffdc20>\u001b[0m in \u001b[0;36mgetDependency\u001b[0;34m(sent, model)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;31m#Find the dependency in the sentance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetDependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlpSpacy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m     \u001b[0mdep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0madjs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.predict\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.greedy_parse\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \"\"\"\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserModel.begin_update\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserStepModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.precompute_hiddens.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/_ml.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnF\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnO\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         )\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mYf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mYf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"/var/www/DataSets/IMBDData/imdb_master.csv\")\n",
    "negMessages=df[df.label=='neg'].review.values\n",
    "negMessages=negMessages[:10]\n",
    "for i in range(len(negMessages)):\n",
    "    print(getEscalationOrNotMatrix(negMessages[i],\"\",\"\",\"\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
